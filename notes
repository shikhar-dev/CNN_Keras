TUTORIAL AND NOTES:
1. This is a standard dataset from MNIST for digit recognition .Images are 28*28 and 60,000 training examples and 10,000 test examples
2. To load data use mnist.load_data as this is already stored in keras.
3. Reshaping data as we are following 'channels last' format. Channels here refers to colors for eg In RGB image there are 3 channels.
   For greyscale we have one channel only. Channels last means that 'X' wil be of the format [rows][colums][channels]. Where X_train[][][] gives pixel
   intensity for a particular cell [row][column] in the particular channel.
4. Input shape is Rank 4 vector where first dimension if for examples or samples.
5. Keras.utils.to_categorical(y, num_classes=None) : Converts a integers to binary class matrix. For use with categorial_crossentropy
   np.utils.to_categorical eg [1,2,3,4] to [0001,0010,0100,1000] number of bits = number of distincts integers (one hot encoded vectors).
6. Now our y_train and y_test has the shape [examples][classes] where each output is [classes] vector (one hot encoded vector). Thus we have 10
   classes as 10 digits.


# Network Architecture

1. The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5×5 and a
   rectifier activation function. This is the input layer, expecting images with the structure outline above [width][height][pixels].

2. Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2×2.

3. The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order
   to reduce overfitting.

4. Next is a layer that converts the 2D matrix data to a vector called Flatten.It allows the output to be processed by standard fully connected layers.

5. Next a fully connected layer with 128 neurons and rectifier activation function.

6. Finally,the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.

# Plotting

1. Plt.subplot(a,b,c) divides the canvas into a rows and b columns and gives the next plot number 'c'. Numbering starts from top left and continues rowise
   left to right.
2. plt.imshow is used to show image stored as matrix of pixel intensities.
3. plt.imread is opp of this. plt.imread('yolo.png')
4. To show grey scale plt.imshow(matrix,cmap = plt.get_cmap('grey')) cmap is color map
5. Finally plt.show is used to print all printed plots.

# NOTES
1. In argument for model.add(Conv2d()) we have to specify data_format as 'channel' last because of our X_train shape.
2. Maxpooling is used to reduce size of output matrix.
3. Dropout to avoid overfitting.
4. CNN Networks generally start with a Convolution layer followed by pooling layer and these two layers are repeated.
   This results in decrease in [row][column] size and increase in number of channels. Finally we add dropout layer and as the number of features are
   sufficienlty reduced we can use a fully connected network like Dense layer after this.
   SO CNN can be said as 'Preprocessing to reduce features' + 'Simple Network' = CNN .